{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Introduction to using ONE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from one.api import ONE\nimport one.alf.io as alfio\n\none = ONE(base_url='https://openalyx.internationalbrainlab.org', silent=True)\n\n\"\"\"\nThe datasets are organized into directory trees by subject, date and session number.  For a\ngiven session there are data files grouped by object (e.g. 'trials'), each with a specific\nattribute (e.g. 'rewardVolume').  The dataset name follows the pattern 'object.attribute',\nfor example 'trials.rewardVolume'.\n\nAn experiment ID (eid) is a string that uniquely identifies a session, for example a combination\nof subject date and number (e.g. KS023/2019-12-10/001), a file path (e.g.\n'C:\\\\Users\\\\Subjects\\\\KS023\\\\2019-12-10\\\\001'), or a UUID (aad23144-0e52-4eac-80c5-c4ee2decb198).\n\nIf the data don't exist locally, they will be downloaded, then loaded.\n\"\"\"\n\n# To load all the data for a given object, use the load_object method:\neid = 'KS023/2019-12-10/001'  # subject/date/number\ntrials = one.load_object(eid, 'trials')  # Returns a dict-like object of numpy arrays\n# The attributes of the returned object mirror the datasets:\nprint(trials.rewardVolume[:5])\n# These can also be accessed with dictionary syntax:\nprint(trials['rewardVolume'][:5])\n# All arrays in the object have the same length (the size of the first dimension) and can\n# therefore be converted to a DataFrame:\ntrials.to_df().head()\n\n\"\"\"\nDatasets can be individually downloaded using the load_dataset method.  This\nfunction takes an experiment ID and a dataset name as positional args.\n\"\"\"\nreward_volume = one.load_dataset(eid, '_ibl_trials.rewardVolume.npy')  # c.f. load_object, above\n\n# To list the datasets available for a given session:\ndsets = one.list_datasets(eid, details=False)\n\n# If connected to a remote database you can get documentation on a dataset\none.describe_dataset(dsets[0])  # alf/_ibl_trials.choice.npy\n# e.g. prints 'which choice was made in choiceworld: -1 (turn CCW), +1 (turn CW), or 0 (nogo)'\n\n\"\"\"\nCollections\n\nFor any given session there may be multiple datasets with the same name that are organized into\nseparate subfolders called collections.  For example there may be spike times for two probes, one\nin 'alf/probe00/spikes.times.npy', the other in 'alf/probe01/spikes.times.npy'.  In IBL, the 'alf'\ndirectory (for ALyx Files) contains the main datasets that people use.  Raw data is in other\ndirectories.\n\nIn this case you must specify the collection when multiple matching datasets are found:\n\"\"\"\nprobe1_spikes = one.load_dataset(eid, 'spikes.times.npy', collection='alf/probe01')\n\n\"\"\"\nLoading with file name parts\n\nYou may also specify specific parts of the filename for even more specific filtering.  Here a\nlist of options will be treated as a logical OR\n\"\"\"\ndataset = dict(object='spikes', attribute='times', extension=['npy', 'bin'])\nprobe1_spikes = one.load_dataset(eid, dataset, collection='alf/probe01')\n\n\"\"\"\nRevisions\n\nRevisions provide an optional way to organize data by version.  The version label is\narbitrary, however the folder must start and end with pound signs and is typically an ISO date,\ne.g. \"#2021-01-01#\". Unlike collections, if a specified revision is not found, the previous\nrevision will be returned.  The revisions are ordered lexicographically.\n\nprobe1_spikes = one.load_dataset(eid, 'trials.intervals.npy', revision='2021-03-15a')\n\"\"\"\n\n\"\"\"\nDownload only\n\nBy default the load methods will download any missing data, then load and return the data.\nWhen the 'download_only' kwarg is true, the data are not loaded.  Instead a list of file paths\nare returned, and any missing datasets are represented by None.\nTODO Revisions should always be dated\n\"\"\"\nfiles = one.load_object(eid, 'trials', download_only=True)\n\n\"\"\"\nYou can load objects and datasets from a file path\n\"\"\"\ntrials = one.load_object(files[0], 'trials')\ncontrast_left = one.load_dataset(files[0], files[0].name)\n\n\"\"\"\nLoading with timeseries\n\nFor loading a dataset along with its timestamps, alf.io.read_ts can be used. It requires a\nfilepath as input.\n\"\"\"\nfiles = one.load_object(eid, 'spikes', collection='alf/probe01', download_only=True)\nts, clusters = alfio.read_ts(files[1])\n\n\"\"\"\nFiltering attributes\n\nTo download and load only a subset of attributes, you can provide a list to the attribute kwarg.\n\"\"\"\nspikes = one.load_object(eid, 'spikes', collection='alf/probe01', attribute=['time*', 'clusters'])\nassert 'amps' not in spikes\n\n\n\"\"\"\nFor any given object the first dimension of every attribute should match in length.  For\nanalysis you can assert that the dimensions match using the check_dimensions property:\n\"\"\"\nassert trials.check_dimensions == 0\n\n# Load spike times from a probe UUID\npid = 'b749446c-18e3-4987-820a-50649ab0f826'\nsession, probe = one.pid2eid(pid)\nspikes_times = one.load_dataset(session, 'spikes.times.npy', collection=f'alf/{probe}')\n\n# List all probes for a session\nprint([x for x in one.list_collections(session) if 'alf/probe' in x])\n\n\"\"\"\nAdvanced loading:\n\nThe load methods require an exact match, therefore `one.load_dataset(eid, 'spikes.times')` will\nraise an exception because 'spikes.times' does not exactly match 'spikes.times.npy'.\nLikewise `one.load_object(eid, 'trial')` will fail because 'trial' != 'trials'.\n\nLoading can be done using unix shell style wildcards, allowing you to load objects and datasets\nthat match a particular pattern.\n\nIf you set the wildcards property of One to False, loading will be done using regular expressions,\nallowing more powerful pattern matching.\n\nBelow is table showing how to express unix style wildcards as a regular expression:\n\n Regex   |    Wildcard    |         Description        |    Example\n-----------------------------------------------------------------------\n   .*            *          Match zero or more chars     spikes.times.*\n   .?            ?          Match one char               timestamps.?sv\n   []            []         Match a range of chars       obj.attr.part[0-9].npy\n\nExamples:\n    spikes.times.* (regex), spikes.times* (wildcard) matches...\n        spikes.times.npy\n        spikes.times\n        spikes.times_ephysClock.npy\n        spikes.times.bin\n\n    clusters.uuids..?sv (regex), clusters.uuids.?sv (wildcard) matches...\n        clusters.uuids.ssv\n        clusters.uuids.csv\n\n    alf/probe0[0-5] (regex), alf/probe0[0-5] (wildcard) matches...\n        alf/probe00\n        alf/probe01\n        [...]\n        alf/probe05\n\"\"\"\n\n# More regex examples\none.wildcards = False\n\n# Load specific attributes from an object ('|' represents a logical OR in regex)\nspikes = one.load_object(eid, 'spikes', collection='alf/probe01', attribute='times|clusters')\nassert 'amps' not in spikes\n\n# Load a dataset ignoring any namespace or extension:\nspike_times = one.load_dataset(eid, '.*spikes.times.*', collection='alf/probe01')\n\n# List all datasets in any probe collection (matches 0 or more of any number)\ndsets = one.list_datasets(eid, collection='alf/probe[0-9]*')\n\n# Load object attributes that are not delimited text files (i.e. tsv, ssv, csv, etc.)\nfiles = one.load_object(eid, 'clusters', extension='[^sv]*', download_only=True)\nassert not any(str(x).endswith('csv') for x in files)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}